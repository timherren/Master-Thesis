# Copy this file to .env and adjust if needed.
# Ollama runs locally â€” no API key required.

# Ollama server URL (use http://ollama:11434 in Docker Compose)
OLLAMA_HOST=http://localhost:11434

# Model for CI summaries
OLLAMA_MODEL=llama3.2:latest

# Model for DAG proposals (larger, better domain knowledge)
OLLAMA_DAG_MODEL=llama3.2:latest
